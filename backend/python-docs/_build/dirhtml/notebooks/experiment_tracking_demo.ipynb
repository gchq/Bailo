{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking with Bailo & MLFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to core concepts covered in previous notebooks, Bailo also offers integrations that might be useful within the wider machine learning lifecycle. This example notebook will run through **experiment tracking** in particular, integrating with **MLFlow Tracking**. The following concepts will be covered:\n",
    "\n",
    "* Creating a new experiment using a Bailo model.\n",
    "* Conducting experiment runs and logging parameters/metrics.\n",
    "* Importing existing experiments from **MLFlow Tracking**.\n",
    "* Publishing results to the Bailo service.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Completion of the basic notebooks, in particular **models_and_releases_demo_pytorch.ipynb**.\n",
    "* Python 3.8.1 or higher (including a notebook environment for this demo).\n",
    "* A local or remote Bailo service (see https://github.com/gchq/Bailo).\n",
    "* Dependencies for MLFlow.\n",
    "* A local or remote MLFlow Tracking server, if following the MLFlow steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Connecting with Bailo\n",
    "\n",
    "In order to create an `Experiment()` object, you'll first need to have a Bailo `Model()` object, and thus a defined `Client()` too. We learned how to do this in a previous notebook, but this time we'll create a new model with a custom schema which supports model metrics. *More on that later...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies...\n",
    "! pip install mlflow bailo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary import statements\n",
    "\n",
    "from bailo import Model, Client, Experiment, Schema, SchemaKind\n",
    "import mlflow\n",
    "import random\n",
    "\n",
    "# Instantiating the Bailo client\n",
    "\n",
    "client = Client(\"http://127.0.0.1:8080\") # <- INSERT BAILO URL (if not hosting locally)\n",
    "\n",
    "# Creating a demo model\n",
    "\n",
    "model = Model.create(client=client, name=\"YOLOv5\", description=\"YOLOv5 model for object detection.\", team_id=\"uncategorised\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up MLFlow Tracking\n",
    "\n",
    "In order to complete the integration element of this tutorial, we'll need to set up a local instance of MLFlow Tracking, and create a sample experiment run. *This will not contain any actual model training and is only to demonstrate the functionality of Bailo.*\n",
    "\n",
    "Run `mlflow ui` on the command line. Typically this will run on **localhost:5000** and the UI can be accessed on a browser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a custom schema for tracking\n",
    "\n",
    "The Bailo UI is designed to display metrics in a particular way, therefore you will need to use a schema that supports this. *This is necessary to publish results*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the schema in an external script as it is quite large!\n",
    "%run -i set_schema.py\n",
    "\n",
    "# Assigns a random schema ID\n",
    "schema_id = random.randint(1, 1000000)\n",
    "\n",
    "# Creates the schema on Bailo\n",
    "schema = Schema.create(client=client, schema_id=str(schema_id), name=\"Experiment Tracking\", description=\"Demo tracking schema\", kind=SchemaKind.MODEL, json_schema=json_schema)\n",
    "\n",
    "# Model cards need to be instantiated with their mandatory fields before metrics can be published. \n",
    "model.card_from_schema(schema_id=str(schema_id))\n",
    "new_card = {\n",
    "  'overview': {\n",
    "    'tags': [],\n",
    "    'modelSummary': 'YOLOv5 model for object detection.',\n",
    "  }\n",
    "}\n",
    "model.update_model_card(model_card=new_card)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new experiment\n",
    "\n",
    "Experiments with the Bailo client are created using the `Model.create_experiment()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = model.create_experiment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting experiment runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an experiment with the Bailo python client\n",
    "\n",
    "You can run experiments directly using the Bailo python client as follows.\n",
    "\n",
    "**NOTE**: This will only work for sequential experiment runs, so if you're running experiments in parallel then it would be better to use **MLFlow Tracking**. We'll learn how to import completed experiments from MLFlow in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary params\n",
    "params = {\n",
    "    \"lr\": 0.01,\n",
    "    \"anchor_t\": 4.0,\n",
    "    \"scale\": 0.5,\n",
    "}\n",
    "\n",
    "# Arbitrary metrics\n",
    "metrics = {\n",
    "    \"accuracy\": 0.98,\n",
    "}\n",
    "\n",
    "for x in range(5):\n",
    "    experiment.start_run()\n",
    "    experiment.log_params(params)\n",
    "    experiment.log_metrics(metrics)\n",
    "    experiment.log_artifacts([\"weights.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.publish(mc_loc=\"performance.performanceMetrics\", run_id=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dummy MLFlow experiment run\n",
    "\n",
    "This section conducts an arbitrary experiment run and logs the params/metrics to your local MLFlow server. *We need this for the next section*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting local tracking URI and experiment name\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Demonstator\")\n",
    "\n",
    "# Logging the same metrics to the local MLFlow server\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"accuracy\", 0.86)\n",
    "    mlflow.log_artifact(\"weights.txt\")\n",
    "    mlflow.set_tag(\"Training Info\", \"YOLOv5 Demo Model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing existing experiments from MLFlow into Bailo\n",
    "\n",
    "As previously mentioned, you can import existing experiments into the `Experiment()` class by using the `Experiment.from_mlflow()` method. You must provide the **MLFlow tracking URI** and the experiment ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_mlflow = model.create_experiment()\n",
    "experiment_mlflow.from_mlflow(tracking_uri=\"http://127.0.0.1:5000\", experiment_id=\"\") # <- INSERT MLFLOW EXPERIMENT ID. CAN BE FOUND ON THE UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing results to Bailo\n",
    "\n",
    "Experiment runs can be published to the model card using the `Experiment.publish()` method, **one at a time**. This is because the intended use is only to publish the most successful run.\n",
    "Therefore, you must specify the **run_id** to publish, as well as specify the location of the metrics in your schema (in this case *performance.performanceMetrics* as per the schema we defined earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_mlflow.publish(mc_loc=\"performance.performanceMetrics\", run_id=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, our metrics should now be under the **Performance** tab of your model card on the UI! Additionally, our artifact will have been published as a new release."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
